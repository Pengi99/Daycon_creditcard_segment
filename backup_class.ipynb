{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from tqdm.auto import tqdm  # 진행률 표시용\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class ML:\n",
    "    def __init__(self, months=None):\n",
    "        self.months = months or ['07','08','09','10','11','12']\n",
    "        self.loaded_data = {}\n",
    "        self.le_target = None\n",
    "\n",
    "    # ----------------------------- 1. 데이터 로드 -----------------------------\n",
    "    def data_load(self):\n",
    "        cats = {\n",
    "            \"회원정보\":(\"1.회원정보\",\"회원정보\",\"customer\"),\n",
    "            \"신용정보\":(\"2.신용정보\",\"신용정보\",\"credit\"),\n",
    "            \"승인매출정보\":(\"3.승인매출정보\",\"승인매출정보\",\"sales\"),\n",
    "            \"청구정보\":(\"4.청구입금정보\",\"청구정보\",\"billing\"),\n",
    "            \"잔액정보\":(\"5.잔액정보\",\"잔액정보\",\"balance\"),\n",
    "            \"채널정보\":(\"6.채널정보\",\"채널정보\",\"channel\"),\n",
    "            \"마케팅정보\":(\"7.마케팅정보\",\"마케팅정보\",\"marketing\"),\n",
    "            \"성과정보\":(\"8.성과정보\",\"성과정보\",\"performance\"),\n",
    "        }\n",
    "        print(\"▶ Loading data...\")\n",
    "        for split in tqdm([\"train\",\"test\"], desc=\"Splits\"):\n",
    "            for folder, suffix, prefix in cats.values():\n",
    "                for m in self.months:\n",
    "                    fp = f\"./{split}/{folder}/2018{m}_{split}_{suffix}.parquet\"\n",
    "                    key = f\"{prefix}_{split}_{m}\"\n",
    "                    try:\n",
    "                        self.loaded_data[key] = pd.read_parquet(fp)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        print(\"✔ Data load complete\\n\")\n",
    "        gc.collect()\n",
    "\n",
    "    # ----------------------------- 2. 전처리 -----------------------------\n",
    "    def data_preprocessing(self, select_features=False, slice_n=1, NA_ratio=0.2):\n",
    "        cats = [\"customer\",\"credit\",\"sales\",\"billing\",\n",
    "                \"balance\",\"channel\",\"marketing\",\"performance\"]\n",
    "        train_dfs, test_dfs = {}, {}\n",
    "\n",
    "        print(\"▶ Preprocessing TRAIN data...\")\n",
    "        for p in tqdm(cats, desc=\"Categories\"):\n",
    "            chunks = []\n",
    "            for m in self.months:\n",
    "                k = f\"{p}_train_{m}\"\n",
    "                if k in self.loaded_data:\n",
    "                    dfm = self.loaded_data.pop(k)\n",
    "                    rows = len(dfm) if slice_n <= 1 else max(1, len(dfm)//slice_n)\n",
    "                    chunks.append(dfm.iloc[:rows])\n",
    "            if not chunks:\n",
    "                continue\n",
    "            df = pd.concat(chunks, axis=0)\n",
    "            df = df.drop(columns=df.columns[df.isna().mean() > NA_ratio])\n",
    "            for c in df.columns:\n",
    "                if df[c].isna().any():\n",
    "                    if is_numeric_dtype(df[c]):\n",
    "                        df[c] = df[c].fillna(df[c].median())\n",
    "                    else:\n",
    "                        mode_val = df[c].mode()\n",
    "                        v = mode_val.iloc[0] if not mode_val.empty else \"MISSING\"\n",
    "                        df[c] = df[c].fillna(v)\n",
    "            train_dfs[f\"{p}_train_df\"] = df\n",
    "\n",
    "        # ------------------ 피처 셀렉션 적용 ------------------\n",
    "        if select_features:\n",
    "            sel = pd.read_csv(\"selected_features.csv\")[\"feature\"].tolist()\n",
    "            for k, v in train_dfs.items():\n",
    "                # CSV 선택 피처 + 키·타깃 컬럼\n",
    "                keep = [c for c in sel if c in v.columns] + [\"기준년월\", \"ID\", \"Segment\"]\n",
    "                # 실제 존재하는 컬럼만, 중복 제거\n",
    "                keep = [c for c in keep if c in v.columns]\n",
    "                keep = list(dict.fromkeys(keep))\n",
    "                train_dfs[k] = v[keep]\n",
    "\n",
    "        train_df = self._merge(train_dfs, \"train\")\n",
    "\n",
    "        print(\"\\n▶ Preprocessing TEST data...\")\n",
    "        for p in tqdm(cats, desc=\"Categories\"):\n",
    "            chunks = []\n",
    "            for m in self.months:\n",
    "                k = f\"{p}_test_{m}\"\n",
    "                if k in self.loaded_data:\n",
    "                    dfm = self.loaded_data.pop(k)\n",
    "                    rows = len(dfm) if slice_n <= 1 else max(1, len(dfm)//slice_n)\n",
    "                    chunks.append(dfm.iloc[:rows])\n",
    "            if not chunks:\n",
    "                continue\n",
    "            df = pd.concat(chunks, axis=0)\n",
    "            extra = [c for c in df.columns if c not in train_df.columns]\n",
    "            if extra:\n",
    "                df = df.drop(columns=extra)\n",
    "            test_dfs[f\"{p}_test_df\"] = df\n",
    "\n",
    "        test_df = self._merge(test_dfs, \"test\")\n",
    "        for c in test_df.columns:\n",
    "            if test_df[c].isna().any():\n",
    "                if is_numeric_dtype(test_df[c]):\n",
    "                    test_df[c] = test_df[c].fillna(test_df[c].median())\n",
    "                else:\n",
    "                    mode_val = test_df[c].mode()\n",
    "                    v = mode_val.iloc[0] if not mode_val.empty else \"MISSING\"\n",
    "                    test_df[c] = test_df[c].fillna(v)\n",
    "\n",
    "        print(\"✔ Preprocessing complete\\n\")\n",
    "        return train_df, test_df\n",
    "\n",
    "    # ----------------------------- 안전한 병합 -----------------------------\n",
    "    def _merge(self, dct, prefix):\n",
    "        order = [\"customer\",\"credit\",\"sales\",\"billing\",\n",
    "                 \"balance\",\"channel\",\"marketing\",\"performance\"]\n",
    "        base = dct.get(f\"customer_{prefix}_df\")\n",
    "        if base is None:\n",
    "            raise ValueError(\"Missing customer base DataFrame for merge\")\n",
    "        for p in order[1:]:\n",
    "            key = f\"{p}_{prefix}_df\"\n",
    "            if key not in dct:\n",
    "                continue\n",
    "            df = dct[key]\n",
    "            # 키 컬럼 보존 여부 확인\n",
    "            if not {'기준년월','ID'}.issubset(df.columns):\n",
    "                print(f\"[WARN] {key} missing merge keys, skipped\")\n",
    "                continue\n",
    "            base = base.merge(df, on=['기준년월','ID'], how='left')\n",
    "        print(f\"{prefix.upper()} merged shape: {base.shape}\\n\")\n",
    "        return base\n",
    "\n",
    "    # ----------------------------- 3. 인코딩 -----------------------------\n",
    "    def data_encoding(self, train_df, test_df, encoding=\"label\"):\n",
    "        feats = [c for c in train_df.columns if c not in (\"ID\",\"Segment\")]\n",
    "        X = train_df[feats].copy()\n",
    "        y = train_df[\"Segment\"].copy()\n",
    "        self.le_target = LabelEncoder().fit(y)\n",
    "        y_enc = self.le_target.transform(y)\n",
    "\n",
    "        cat_cols = X.select_dtypes(\"object\").columns.tolist()\n",
    "        if encoding == \"label\":\n",
    "            enc = {c: LabelEncoder().fit(X[c]) for c in cat_cols}\n",
    "            for c, e in enc.items():\n",
    "                X[c] = e.transform(X[c])\n",
    "            X_test = test_df.drop(columns=\"ID\", errors=\"ignore\").copy()\n",
    "            for c, e in enc.items():\n",
    "                unseen = set(X_test[c]) - set(e.classes_)\n",
    "                if unseen:\n",
    "                    e.classes_ = np.append(e.classes_, list(unseen))\n",
    "                X_test[c] = e.transform(X_test[c])\n",
    "        else:\n",
    "            ct = ColumnTransformer(\n",
    "                [(\"ohe\", OneHotEncoder(handle_unknown='ignore'), cat_cols)],\n",
    "                remainder='passthrough'\n",
    "            )\n",
    "            X = pd.DataFrame(ct.fit_transform(X), columns=ct.get_feature_names_out())\n",
    "            X_test = pd.DataFrame(\n",
    "                ct.transform(test_df.drop(columns=\"ID\", errors=\"ignore\")),\n",
    "                columns=ct.get_feature_names_out()\n",
    "            )\n",
    "\n",
    "        print(\"✔ Encoding complete\\n\")\n",
    "        return X, y_enc, X_test\n",
    "\n",
    "    # ----------------------------- 4. 모델 학습 -----------------------------\n",
    "    def train_model(self, X, y, X_test, test_df,\n",
    "                    method, file_name=\"submit\",\n",
    "                    use_gpu=True, tune=True, val_ratio=0.3):\n",
    "\n",
    "        if method == 1:   # XGBoost\n",
    "            base = xgb.XGBClassifier(\n",
    "                random_state=42, eval_metric=\"mlogloss\",\n",
    "                tree_method=(\"gpu_hist\" if use_gpu else \"hist\"),\n",
    "                predictor=(\"gpu_predictor\" if use_gpu else \"auto\"),\n",
    "                gpu_id=(0 if use_gpu else -1)\n",
    "            )\n",
    "            grid = {\"max_depth\":[3,5], \"learning_rate\":[0.05,0.1], \"n_estimators\":[300,500]}\n",
    "        elif method == 2: # CatBoost\n",
    "            base = CatBoostClassifier(\n",
    "                random_state=42, verbose=False,\n",
    "                task_type=(\"GPU\" if use_gpu else \"CPU\"),\n",
    "                devices=(\"0\" if use_gpu else None)\n",
    "            )\n",
    "            grid = {\"depth\":[4,6], \"learning_rate\":[0.05,0.1], \"iterations\":[400,600]}\n",
    "        elif method == 3: # RandomForest (CPU only)\n",
    "            base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "            grid = {\"n_estimators\":[300,500], \"max_depth\":[None,20], \"min_samples_leaf\":[1,3]}\n",
    "        else:\n",
    "            raise ValueError(\"method must be 1, 2, or 3\")\n",
    "\n",
    "        print(\"▶ Starting training...\")\n",
    "        if tune and grid:\n",
    "            gs = GridSearchCV(\n",
    "                base, grid,\n",
    "                cv=StratifiedKFold(5, shuffle=True, random_state=42),\n",
    "                scoring=\"f1_macro\", n_jobs=-1, verbose=2\n",
    "            ).fit(X, y)\n",
    "            model = gs.best_estimator_\n",
    "            print(\"✔ Best hyperparams:\", gs.best_params_)\n",
    "        else:\n",
    "            model = base.fit(X, y)\n",
    "\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X, y, test_size=val_ratio, random_state=42, stratify=y\n",
    "        )\n",
    "        model.fit(X_tr, y_tr)\n",
    "        print(\"▶ Validation results:\")\n",
    "        print(classification_report(y_val, model.predict(X_val)))\n",
    "\n",
    "        if not (tune and grid):\n",
    "            model.fit(X, y)\n",
    "\n",
    "        X_test = X_test.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "        preds = self.le_target.inverse_transform(model.predict(X_test))\n",
    "        out = (\n",
    "            test_df.copy().reset_index(drop=True)\n",
    "            .assign(pred=preds)\n",
    "            .groupby(\"ID\")[\"pred\"].agg(lambda s: s.value_counts().idxmax())\n",
    "            .reset_index().rename(columns={\"pred\":\"Segment\"})\n",
    "        )\n",
    "        out.to_csv(f\"{file_name}.csv\", index=False)\n",
    "        print(f\"✔ Model saved → {file_name}.csv\\n\")\n",
    "        gc.collect()\n",
    "\n",
    "    # ----------------------------- 5. 피처 선택 -----------------------------\n",
    "    def select_features(self, X, y, top_n=400, corr_threshold=0.9):\n",
    "        print(\"▶ Calculating feature importances...\")\n",
    "        rf = RandomForestClassifier(n_estimators=400, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X, y)\n",
    "        imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "        # 전체 피처 중요도 CSV 저장\n",
    "        imp_df = imp.reset_index()\n",
    "        imp_df.columns = ['feature','importance']\n",
    "        imp_df.to_csv(\"feature_importances.csv\", index=False)\n",
    "        print(f\"✔ Saved full importances ({len(imp_df)}) to feature_importances.csv\")\n",
    "\n",
    "        # 상위 top_n 필터링\n",
    "        top_feats = imp.head(top_n).index.tolist()\n",
    "        print(f\"▶ Filtering top {top_n} features by correlation (threshold={corr_threshold})\")\n",
    "        corr = X[top_feats].corr().abs()\n",
    "        selected = []\n",
    "        total = len(top_feats)\n",
    "        for i, f in enumerate(top_feats, 1):\n",
    "            print(f\"   Progress {i}/{total}: {f}\", end=\"\\r\")\n",
    "            if all(corr.loc[f, s] <= corr_threshold for s in selected):\n",
    "                selected.append(f)\n",
    "        print()\n",
    "\n",
    "        # 최종 선택 피처 CSV 저장\n",
    "        sel_df = pd.DataFrame(selected, columns=[\"feature\"])\n",
    "        sel_df.to_csv(\"selected_features.csv\", index=False)\n",
    "        print(f\"✔ Saved {len(selected)} selected features to selected_features.csv\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = ML()\n",
    "ml.data_load()                       # 모든 parquet 로드\n",
    "\n",
    "\n",
    "train_sub, test_sub = ml.data_preprocessing(\n",
    "    select_features=False, slice_n=10, NA_ratio=0.2\n",
    ")\n",
    "X_sub, y_sub, X_test_dummy = ml.data_encoding(train_sub, test_sub)\n",
    "\n",
    "\n",
    "# RandomForest + 하이퍼파라미터 서치로 중요도 기반 상위 500개 저장\n",
    "ml.select_features(\n",
    "    X=X_sub, y=y_sub,\n",
    "    top_n=500,\n",
    "    corr_threshold=0.9             # 내부에서 selected_features.csv 생성\n",
    ")\n",
    "\n",
    "ml.data_load()\n",
    "train_df, test_df = ml.data_preprocessing(\n",
    "    select_features=True,    # 방금 저장한 CSV를 사용\n",
    "    slice_n=5,\n",
    "    NA_ratio=0.2\n",
    ")\n",
    "X, y, X_test = ml.data_encoding(train_df, test_df)\n",
    "\n",
    "\n",
    "ml.train_model(\n",
    "    X=X, y_encoded=y,\n",
    "    X_test=X_test,\n",
    "    method=1,               # 1:XGBoost  2:CatBoost  3:RandomForest\n",
    "    test_df=test_df,\n",
    "    file_name=\"submit\", # 저장 파일명\n",
    "    val_ratio=0.3,\n",
    "    tune=True               # GridSearchCV 사용\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c0ce2e655842c6883821f85ee82ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splits:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Data load complete\n",
      "\n",
      "▶ Preprocessing TRAIN data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871331e7cd644feca95a4ab0a25f2f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Categories:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN merged shape: (480000, 272)\n",
      "\n",
      "\n",
      "▶ Preprocessing TEST data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2889bdb104af4d5ca1b24e3b039a1700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Categories:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST merged shape: (120000, 271)\n",
      "\n",
      "✔ Preprocessing complete\n",
      "\n",
      "✔ Encoding complete\n",
      "\n",
      "▶ Starting training...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "✔ Best hyperparams: {'max_depth': None, 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "▶ Validation results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.44      0.61        68\n",
      "           1       1.00      0.29      0.44         7\n",
      "           2       0.91      0.72      0.81      7668\n",
      "           3       0.86      0.75      0.80     20965\n",
      "           4       0.95      0.99      0.97    115292\n",
      "\n",
      "    accuracy                           0.94    144000\n",
      "   macro avg       0.94      0.64      0.73    144000\n",
      "weighted avg       0.94      0.94      0.94    144000\n",
      "\n",
      "✔ Model saved → submit.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml = ML()\n",
    "ml.data_load()\n",
    "train_df, test_df = ml.data_preprocessing(\n",
    "    select_features=True,    # 방금 저장한 CSV를 사용\n",
    "    slice_n=5,\n",
    "    NA_ratio=0.2\n",
    ")\n",
    "X, y, X_test = ml.data_encoding(train_df, test_df)\n",
    "\n",
    "\n",
    "ml.train_model(\n",
    "    X=X, y=y,\n",
    "    X_test=X_test,\n",
    "    method=3,               # 1:XGBoost  2:CatBoost  3:RandomForest\n",
    "    test_df=test_df,\n",
    "    file_name=\"submit\", # 저장 파일명\n",
    "    val_ratio=0.3,\n",
    "    tune=True               # GridSearchCV 사용\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
